# Dose Response Curve Example

In this tutorial, we will demonstrate how to estimate the Dose Response Curve (or Average Dose Response Function - ADRF) using `skcausal`. We will use a synthetic dataset where the true causal effect is known, allowing us to compare different estimation methods.

## Data Generation

First, we generate a synthetic dataset. This dataset includes covariates ($X$), a continuous treatment ($t$), and an outcome ($y$). We use `SyntheticDataset2` which simulates a scenario with confounding variables.

```{python}
from skcausal.datasets.synthetic2 import SyntheticDataset2

dataset = SyntheticDataset2(n_features=6)
dataset.prepare(n=1000, split_seed=42, preparation_seed=42)

X, t, y = dataset.retrieve()


X
```

## The True Average Dose Response Function (ADRF)

Since this is synthetic data, we can calculate the true ADRF. This represents the actual expected outcome for each treatment level if we were to assign that treatment to the entire population. This serves as our ground truth for evaluation.

```{python}
import numpy as np
import polars as pl

treatment_grid = pl.DataFrame(np.linspace(0, 1, 100), schema=t.schema)
true_adrf = dataset.get_adrf(X, t)
```

## Visualizing the Data

Let's visualize the observed data (scatter plot) and the true ADRF (red line). The scatter plot shows the observed outcomes for given treatments, which are influenced by confounding. The red line shows the underlying causal relationship we want to recover.

```{python}

import matplotlib.pyplot as plt

fig, ax = plt.subplots()
plt.scatter(t, y, alpha=0.3)
# Argsort true_adrf for plotting
sorted_indices = np.argsort(t.to_numpy().flatten())
plt.plot(
    t[sorted_indices],
    true_adrf[sorted_indices],
    color="red",
    linewidth=2,
)
fig.show()
```

## 1. Naive Approach: Direct Estimation (Ignoring Covariates)

As a baseline, we first use a "direct" estimator that ignores covariates. It simply models $Y$ as a function of $T$, $E[Y|T]$. In the presence of confounding, this estimator is likely to be biased because it mistakes correlation for causation.

```{python}
from skcausal.causal_estimators.ignore_covariates import DirectNoCovariates

from sklearn.ensemble import RandomForestRegressor

estimator = DirectNoCovariates(outcome_regressor=RandomForestRegressor(min_samples_leaf=30))
estimator.fit(X, t=t, y=y)
```

```{python}
y_pred = estimator.predict_adrf(X, t)
```

We can now plot the predictions of this naive estimator (green) against the true ADRF (red).

```{python}

fig, ax = plt.subplots()
plt.scatter(t, y, alpha=0.3)
# Argsort true_adrf for plotting
sorted_indices = np.argsort(t.to_numpy().flatten())
plt.plot(
    t[sorted_indices],
    true_adrf[sorted_indices],
    color="red",
    linewidth=2,
)
plt.plot(
    t[sorted_indices],
    y_pred[sorted_indices],
    color="green",
    linewidth=2,
)
fig.show()
```

## 2. Doubly Robust Estimation

Now we employ a more rigorous method: **Doubly Robust (DR) Estimation**. This approach combines two models:
1.  A **propensity model** (treatment model) that estimates the probability density of the treatment given covariates.
2.  An **outcome model** that estimates the outcome given treatment and covariates.

The DR estimator is consistent if *either* the propensity model *or* the outcome model is correctly specified. Here, we use `DoublyRobustPseudoOutcome`.

```{python}
from skcausal.causal_estimators.continuous.doubly_robust import (
    DoublyRobustPseudoOutcome,
)
from skcausal.weight_estimators.multiplicative_boosting import (
    DiscriminativeWeightBoosting,
)
from sklearn.ensemble import RandomForestClassifier

dr = DoublyRobustPseudoOutcome(
    treatment_regressor=DiscriminativeWeightBoosting(
        classifier=RandomForestClassifier(min_samples_leaf=30),
        n_boosting_iter=5,
        method="balanced",
        complexity_factor=5,
    ),
    outcome_regressor=RandomForestRegressor(min_samples_leaf=30),
    pseudo_outcome_regressor=RandomForestRegressor(min_samples_leaf=30),
)
dr.fit(X, t=t, y=y)

```

```{python}
y_dr_pred = dr.predict_adrf(X, t)
```

## 3. Generalized Propensity Score (GPS)

The **Generalized Propensity Score (GPS)** method adjusts for confounding by conditioning on the generalized propensity score. It involves:
1.  Estimating the conditional density of the treatment given covariates (the GPS).
2.  Estimating the conditional expectation of the outcome given the treatment and the GPS.
3.  Averaging over the GPS to estimate the ADRF.

```{python}
from skcausal.causal_estimators.gps import GPS

gps = GPS(
    treatment_regressor=DiscriminativeWeightBoosting(
        classifier=RandomForestClassifier(min_samples_leaf=30),
        n_boosting_iter=5,
        method="uniform",
        complexity_factor=5,
    ),
    outcome_regressor=RandomForestRegressor(min_samples_leaf=30),
    include_in_outcome_dataset="both",
    predict_subsample_size=100,
)
```


```{python}
gps.fit(X, t=t, y=y)
y_gps_pred = gps.predict_adrf(X, t)
```

## 4. Propensity Weighting (Pseudo-Outcome)

Another approach is **Propensity Weighting**. This method uses the inverse of the propensity score (or related weights) to re-weight the data, creating a pseudo-population where the treatment is independent of covariates. We use `PropensityPseudoOutcomeContinuous` here.

```{python}
from skcausal.causal_estimators.continuous import PropensityPseudoOutcomeContinuous
from sklearn.neural_network import MLPClassifier

pw = PropensityPseudoOutcomeContinuous(
    treatment_regressor=DiscriminativeWeightBoosting(
        classifier=MLPClassifier(hidden_layer_sizes=(25, 25), max_iter=500),
        n_boosting_iter=10,
        method="uniform",
        complexity_factor=1.1,
    ),
    pseudo_outcome_regressor=RandomForestRegressor(min_samples_leaf=30),
)
```

```{python}
pw.fit(X, t=t, y=y)
y_pw_pred = pw.predict_adrf(X, t)
```

## Final Comparison

Finally, let's compare all the estimated curves against the true ADRF. This plot will show how well each method recovers the true causal effect compared to the naive baseline.

```{python}

fig, ax = plt.subplots()
plt.scatter(t, y, alpha=0.3)
# Argsort true_adrf for plotting
sorted_indices = np.argsort(t.to_numpy().flatten())
plt.plot(
    t[sorted_indices],
    true_adrf[sorted_indices],
    color="red",
    linewidth=2,
)
plt.plot(
    t[sorted_indices],
    y_pred[sorted_indices],
    color="green",
    linewidth=2,
    label="DirectNoCovariates",
)
plt.plot(
    t[sorted_indices],
    y_dr_pred[sorted_indices],
    color="blue",
    linewidth=2,
    label="DoublyRobustPseudoOutcome",
)
plt.plot(
    t[sorted_indices],
    y_gps_pred[sorted_indices],
    color="orange",
    linewidth=2,
    label="GPS",
)
plt.plot(
    t[sorted_indices],
    y_pw_pred[sorted_indices],
    color="purple",
    linewidth=2,
    label="PropensityPseudoOutcomeContinuous",
)
plt.legend()
fig.show()
```